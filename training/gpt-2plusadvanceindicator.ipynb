{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import GPT2Tokenizer, GPT2Model\nimport torch\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding, Dropout\nfrom keras.optimizers import Adam\n\n# Assuming your DataFrame is named 'df'\ndf = pd.read_csv('/kaggle/input/dataset/combined_DJIA_NEWS.csv')\n# Drop unnecessary columns for training\ndf.drop(['Unnamed: 0', 'Date'], axis=1, inplace=True)\n\n# Create additional features from numerical data\ndf['Lowest_Low'] = df['Low'].rolling(window=14).min()\ndf['Highest_High'] = df['High'].rolling(window=14).max()\ndf['Stochastic_K'] = ((df['Close'] - df['Lowest_Low']) / (df['Highest_High'] - df['Lowest_Low'])) * 100\ndf['Stochastic_D'] = df['Stochastic_K'].rolling(window=3).mean()\ndf['Momentum'] = df['Close'] - df['Close'].shift(10)\ndf['Rate_of_Change'] = (df['Close'] / df['Close'].shift(10)) * 100\ndf['William_R'] = ((df['Highest_High'] - df['Close']) / (df['Highest_High'] - df['Lowest_Low'])) * -100\ndf['A/D_Oscillator'] = (df['High'] - df['Close'].shift()) / (df['High'] - df['Low'])\ndf['Close_MA_5'] = df['Close'].rolling(window=5).mean()\ndf['Disparity_5'] = ((df['Close'] - df['Close_MA_5']) / df['Close_MA_5']) * 100\ndf = df.drop(columns=['Lowest_Low', 'Highest_High', 'Close_MA_5'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T13:35:21.357680Z","iopub.execute_input":"2024-02-08T13:35:21.358063Z","iopub.status.idle":"2024-02-08T13:35:42.619783Z","shell.execute_reply.started":"2024-02-08T13:35:21.358031Z","shell.execute_reply":"2024-02-08T13:35:42.618644Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-08 13:35:32.334284: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-08 13:35:32.334390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-08 13:35:32.539271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split the data into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Load pre-trained GPT-2 tokenizer and model from Hugging Face Model Hub\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nimport numpy as np\n\nclass GPT2EmbeddingTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        embeddings = []\n        for row in X.values:\n            # Convert each element to string\n            row = [str(element) for element in row]\n\n            # Combine headlines into a single string\n            combined_headlines = \" \".join(row)\n\n            # Tokenize and get embeddings\n            inputs = tokenizer(combined_headlines, return_tensors=\"pt\", truncation=True)\n            outputs = model(**inputs)\n            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n            embeddings.append(embedding)\n\n        return np.array(embeddings)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T13:35:59.651097Z","iopub.execute_input":"2024-02-08T13:35:59.652024Z","iopub.status.idle":"2024-02-08T13:36:04.308217Z","shell.execute_reply.started":"2024-02-08T13:35:59.651991Z","shell.execute_reply":"2024-02-08T13:36:04.307381Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f9ea66b0cf42e585bcf3d006272383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4001a7907974ef1a988d4ac3528527f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffe5103088c47bebadefd99ce15ce43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085472ec6b5b4c9f85351b4d9d473f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47c14879d71d4dd28b12abf9a51a7a23"}},"metadata":{}}]},{"cell_type":"code","source":"# Define the features and the target variable\ntext_features = ['Top' + str(i) for i in range(1, 26)]\nnumerical_features = ['Stochastic_K', 'Stochastic_D', 'Momentum', 'Rate_of_Change', 'William_R', 'A/D_Oscillator', 'Disparity_5']\ntarget_variable = 'Label'\n\n# Define the preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', GPT2EmbeddingTransformer(), text_features),\n        ('num', StandardScaler(), numerical_features)\n    ]\n)\n\n# Separate features and target variable in training and testing sets\nX_train = train_df[text_features + numerical_features]\ny_train = train_df[target_variable]\nX_test = test_df[text_features + numerical_features]\ny_test = test_df[target_variable]\n\n# Transform the data\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T13:36:11.786267Z","iopub.execute_input":"2024-02-08T13:36:11.786648Z","iopub.status.idle":"2024-02-08T14:02:57.044846Z","shell.execute_reply.started":"2024-02-08T13:36:11.786617Z","shell.execute_reply":"2024-02-08T14:02:57.043915Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of X_train_transformed:\", X_train_transformed.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:11:45.258257Z","iopub.execute_input":"2024-02-08T14:11:45.258661Z","iopub.status.idle":"2024-02-08T14:11:45.264015Z","shell.execute_reply.started":"2024-02-08T14:11:45.258633Z","shell.execute_reply":"2024-02-08T14:11:45.262881Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Shape of X_train_transformed: (1590, 775)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**LSTM**","metadata":{}},{"cell_type":"code","source":"# Reshape the data for LSTM\nX_train_transformed = X_train_transformed.reshape((X_train_transformed.shape[0], X_train_transformed.shape[1], 1))\nX_test_transformed = X_test_transformed.reshape((X_test_transformed.shape[0], X_test_transformed.shape[1], 1))\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(X_train_transformed.shape[1], 1), return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Train the model\nmodel.fit(X_train_transformed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Evaluate the model\npredictions = (model.predict(X_test_transformed) > 0.5).astype(int)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T14:11:49.687031Z","iopub.execute_input":"2024-02-08T14:11:49.687423Z","iopub.status.idle":"2024-02-08T14:12:18.915231Z","shell.execute_reply.started":"2024-02-08T14:11:49.687393Z","shell.execute_reply":"2024-02-08T14:12:18.914157Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707401516.463329     129 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"45/45 [==============================] - 8s 68ms/step - loss: nan - accuracy: 0.4738 - val_loss: nan - val_accuracy: 0.5346\nEpoch 2/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 3/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 4/10\n45/45 [==============================] - 2s 47ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 5/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 6/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 7/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 8/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 9/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 10/10\n45/45 [==============================] - 2s 44ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\n13/13 [==============================] - 1s 19ms/step\nAccuracy: 0.44\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nfrom tensorflow.keras.models import load_model\nmodel.save('lstm_model_advance.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:13:42.915577Z","iopub.execute_input":"2024-02-08T14:13:42.915949Z","iopub.status.idle":"2024-02-08T14:13:43.074184Z","shell.execute_reply.started":"2024-02-08T14:13:42.915920Z","shell.execute_reply":"2024-02-08T14:13:43.073183Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Assuming your X_train_transformed and X_test_transformed are 3D arrays\n# (number of samples, number of timesteps, number of features)\n# You can adjust input_shape based on your data\n\n# Define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(X_train_transformed.shape[1], X_train_transformed.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_transformed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Evaluate the model\npredictions = (model.predict(X_test_transformed) > 0.5).astype(int)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:13:52.087326Z","iopub.execute_input":"2024-02-08T14:13:52.087723Z","iopub.status.idle":"2024-02-08T14:14:06.723700Z","shell.execute_reply.started":"2024-02-08T14:13:52.087692Z","shell.execute_reply":"2024-02-08T14:14:06.722706Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 5s 36ms/step - loss: nan - accuracy: 0.4598 - val_loss: nan - val_accuracy: 0.5346\nEpoch 2/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 3/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 4/10\n45/45 [==============================] - 1s 23ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 5/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 6/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 7/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 8/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 9/10\n45/45 [==============================] - 1s 22ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 10/10\n45/45 [==============================] - 1s 23ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\n13/13 [==============================] - 1s 9ms/step\nAccuracy: 0.44\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**LogisticRegression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.impute import SimpleImputer\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\nimputer = SimpleImputer(strategy='mean')\nX_train_flattened = imputer.fit_transform(X_train_flattened)\nX_test_flattened = imputer.transform(X_test_flattened)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_flattened and X_test_flattened are your feature matrices\n# and y_train is your target variable\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Define the Logistic Regression model\nlogreg_model = LogisticRegression(random_state=42)\n\n# Train the model\nlogreg_model.fit(X_train_imputed, y_train)\n\n# Make predictions on training set\nlogreg_train_predictions = logreg_model.predict(X_train_imputed)\n\n# Make predictions on test set\nlogreg_test_predictions = logreg_model.predict(X_test_imputed)\n\n# Evaluate the model on training set\ntrain_accuracy = accuracy_score(y_train, logreg_train_predictions)\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\n\n# Evaluate the model on test set\ntest_accuracy = accuracy_score(y_test, logreg_test_predictions)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display classification report for test set\nprint(\"Classification Report (Test Set):\")\nprint(classification_report(y_test, logreg_test_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:15:32.468363Z","iopub.execute_input":"2024-02-08T14:15:32.469068Z","iopub.status.idle":"2024-02-08T14:15:32.719804Z","shell.execute_reply.started":"2024-02-08T14:15:32.469034Z","shell.execute_reply":"2024-02-08T14:15:32.712624Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training Accuracy: 0.92\nTest Accuracy: 0.87\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n           0       0.85      0.85      0.85       174\n           1       0.88      0.88      0.88       224\n\n    accuracy                           0.87       398\n   macro avg       0.86      0.87      0.86       398\nweighted avg       0.87      0.87      0.87       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"code","source":"\nmodel.save('logistic_model_advance.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:16:42.165252Z","iopub.execute_input":"2024-02-08T14:16:42.165821Z","iopub.status.idle":"2024-02-08T14:16:42.188926Z","shell.execute_reply.started":"2024-02-08T14:16:42.165777Z","shell.execute_reply":"2024-02-08T14:16:42.188221Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Define the SVM model\nsvm_model = SVC(random_state=42)\n\n# Train the model\nsvm_model.fit(X_train_imputed, y_train)\n\n# Make predictions on training set\nsvm_train_predictions = svm_model.predict(X_train_imputed)\n\n# Make predictions on test set\nsvm_test_predictions = svm_model.predict(X_test_imputed)\n\n# Evaluate the model on training set\ntrain_accuracy = accuracy_score(y_train, svm_train_predictions)\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\n\n# Evaluate the model on test set\ntest_accuracy = accuracy_score(y_test, svm_test_predictions)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display classification report for test set\nprint(\"Classification Report (Test Set):\")\nprint(classification_report(y_test, svm_test_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:16:47.822932Z","iopub.execute_input":"2024-02-08T14:16:47.823323Z","iopub.status.idle":"2024-02-08T14:16:50.969611Z","shell.execute_reply.started":"2024-02-08T14:16:47.823293Z","shell.execute_reply":"2024-02-08T14:16:50.968527Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training Accuracy: 0.53\nTest Accuracy: 0.56\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       174\n           1       0.56      1.00      0.72       224\n\n    accuracy                           0.56       398\n   macro avg       0.28      0.50      0.36       398\nweighted avg       0.32      0.56      0.41       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nfrom tensorflow.keras.models import load_model\nmodel.save('supportvector_model_advance.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:17:22.367559Z","iopub.execute_input":"2024-02-08T14:17:22.367937Z","iopub.status.idle":"2024-02-08T14:17:22.390034Z","shell.execute_reply.started":"2024-02-08T14:17:22.367908Z","shell.execute_reply":"2024-02-08T14:17:22.389335Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**svm with window size 25**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_flattened and X_test_flattened are your feature matrices\n# and y_train is your target variable\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Set the training window size\nwindow_size = 25\n\n# Initialize the SVM model\nsvm_model = SVC(random_state=42)\n\n# Train the model with a sliding window\nfor i in range(window_size, len(X_train_imputed)):\n    X_train_window = X_train_imputed[i - window_size:i]\n    y_train_window = y_train[i - window_size:i]\n    svm_model.fit(X_train_window, y_train_window)\n\n# Make predictions on training set\nsvm_train_predictions = svm_model.predict(X_train_imputed)\n\n# Make predictions on test set\nsvm_test_predictions = svm_model.predict(X_test_imputed)\n\n# Evaluate the model on training set\ntrain_accuracy = accuracy_score(y_train, svm_train_predictions)\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\n\n# Evaluate the model on test set\ntest_accuracy = accuracy_score(y_test, svm_test_predictions)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display classification report for test set\nprint(\"Classification Report (Test Set):\")\nprint(classification_report(y_test, svm_test_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:17:28.294726Z","iopub.execute_input":"2024-02-08T14:17:28.295489Z","iopub.status.idle":"2024-02-08T14:17:31.099244Z","shell.execute_reply.started":"2024-02-08T14:17:28.295450Z","shell.execute_reply":"2024-02-08T14:17:31.097980Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training Accuracy: 0.47\nTest Accuracy: 0.44\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**CNN-LSTM**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Reshape back to the original shape\nX_train_imputed = X_train_imputed.reshape(X_train_transformed.shape)\nX_test_imputed = X_test_imputed.reshape(X_test_transformed.shape)\n\n# CNN-LSTM model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_imputed.shape[1], X_train_imputed.shape[2])))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_imputed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Make predictions\npredictions = (model.predict(X_test_imputed) > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:17:38.129765Z","iopub.execute_input":"2024-02-08T14:17:38.130154Z","iopub.status.idle":"2024-02-08T14:21:34.541324Z","shell.execute_reply.started":"2024-02-08T14:17:38.130124Z","shell.execute_reply":"2024-02-08T14:21:34.540272Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 27s 529ms/step - loss: 3657.5452 - accuracy: 0.6499 - val_loss: 0.6617 - val_accuracy: 0.6667\nEpoch 2/10\n45/45 [==============================] - 23s 513ms/step - loss: nan - accuracy: 0.5395 - val_loss: nan - val_accuracy: 0.5346\nEpoch 3/10\n45/45 [==============================] - 23s 512ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 4/10\n45/45 [==============================] - 23s 511ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 5/10\n45/45 [==============================] - 23s 516ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 6/10\n45/45 [==============================] - 23s 510ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 7/10\n45/45 [==============================] - 23s 518ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 8/10\n45/45 [==============================] - 23s 514ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 9/10\n45/45 [==============================] - 23s 509ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 10/10\n45/45 [==============================] - 23s 513ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\n13/13 [==============================] - 1s 65ms/step\nAccuracy: 0.44\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nfrom tensorflow.keras.models import load_model\nmodel.save('lstm_model_advance.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CNN-GRU**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_transformed, X_test_transformed, y_train, and y_test are your data\n# X_train_transformed and X_test_transformed should have shape (samples, timesteps, features)\n\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Reshape back to the original shape\nX_train_imputed = X_train_imputed.reshape(X_train_transformed.shape)\nX_test_imputed = X_test_imputed.reshape(X_test_transformed.shape)\n\n# CNN-GRU model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_imputed.shape[1], X_train_imputed.shape[2])))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(GRU(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_imputed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Make predictions\npredictions = (model.predict(X_test_imputed) > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:21:45.181847Z","iopub.execute_input":"2024-02-08T14:21:45.182206Z","iopub.status.idle":"2024-02-08T14:26:09.458701Z","shell.execute_reply.started":"2024-02-08T14:21:45.182167Z","shell.execute_reply":"2024-02-08T14:26:09.457669Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 30s 595ms/step - loss: 0.6341 - accuracy: 0.6639 - val_loss: 0.5519 - val_accuracy: 0.7421\nEpoch 2/10\n45/45 [==============================] - 26s 580ms/step - loss: 0.4497 - accuracy: 0.8099 - val_loss: 0.3562 - val_accuracy: 0.8868\nEpoch 3/10\n45/45 [==============================] - 26s 579ms/step - loss: 0.3215 - accuracy: 0.8637 - val_loss: 0.2943 - val_accuracy: 0.8994\nEpoch 4/10\n45/45 [==============================] - 26s 577ms/step - loss: 0.2837 - accuracy: 0.8735 - val_loss: 0.2440 - val_accuracy: 0.8994\nEpoch 5/10\n45/45 [==============================] - 26s 572ms/step - loss: 0.2756 - accuracy: 0.8735 - val_loss: 0.2463 - val_accuracy: 0.8994\nEpoch 6/10\n45/45 [==============================] - 26s 577ms/step - loss: 0.2696 - accuracy: 0.8735 - val_loss: 0.2634 - val_accuracy: 0.8616\nEpoch 7/10\n45/45 [==============================] - 26s 576ms/step - loss: 0.2722 - accuracy: 0.8672 - val_loss: 0.2322 - val_accuracy: 0.8931\nEpoch 8/10\n45/45 [==============================] - 26s 578ms/step - loss: 0.2668 - accuracy: 0.8784 - val_loss: 0.2273 - val_accuracy: 0.9182\nEpoch 9/10\n45/45 [==============================] - 26s 568ms/step - loss: 0.2604 - accuracy: 0.8798 - val_loss: 0.2301 - val_accuracy: 0.9119\nEpoch 10/10\n45/45 [==============================] - 26s 571ms/step - loss: 0.2579 - accuracy: 0.8819 - val_loss: 0.2269 - val_accuracy: 0.8994\n13/13 [==============================] - 1s 71ms/step\nAccuracy: 0.89\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.89      0.88       174\n           1       0.91      0.89      0.90       224\n\n    accuracy                           0.89       398\n   macro avg       0.89      0.89      0.89       398\nweighted avg       0.89      0.89      0.89       398\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nfrom tensorflow.keras.models import load_model\nmodel.save('cnnGgru_advance.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:27:48.829642Z","iopub.execute_input":"2024-02-08T14:27:48.830488Z","iopub.status.idle":"2024-02-08T14:27:48.857405Z","shell.execute_reply.started":"2024-02-08T14:27:48.830452Z","shell.execute_reply":"2024-02-08T14:27:48.856452Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**ANN**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_transformed, X_test_transformed, y_train, and y_test are your data\n# X_train_transformed and X_test_transformed should have shape (samples, features)\n\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Create a simple ANN model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X_train_imputed.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_imputed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Make predictions\npredictions = (model.predict(X_test_imputed) > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:27:52.612906Z","iopub.execute_input":"2024-02-08T14:27:52.613425Z","iopub.status.idle":"2024-02-08T14:27:56.153644Z","shell.execute_reply.started":"2024-02-08T14:27:52.613389Z","shell.execute_reply":"2024-02-08T14:27:56.152716Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 2s 8ms/step - loss: 0.8757 - accuracy: 0.5353 - val_loss: 0.7348 - val_accuracy: 0.4780\nEpoch 2/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.5797 - accuracy: 0.7289 - val_loss: 0.5967 - val_accuracy: 0.6226\nEpoch 3/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.5364 - accuracy: 0.7519 - val_loss: 0.5071 - val_accuracy: 0.7987\nEpoch 4/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.4886 - accuracy: 0.7966 - val_loss: 0.4767 - val_accuracy: 0.7862\nEpoch 5/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.8239 - val_loss: 0.4581 - val_accuracy: 0.7862\nEpoch 6/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.4167 - accuracy: 0.8302 - val_loss: 0.3871 - val_accuracy: 0.8365\nEpoch 7/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.8498 - val_loss: 0.3451 - val_accuracy: 0.8868\nEpoch 8/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8735 - val_loss: 0.3195 - val_accuracy: 0.8868\nEpoch 9/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.2998 - accuracy: 0.8889 - val_loss: 0.2958 - val_accuracy: 0.8805\nEpoch 10/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.3158 - accuracy: 0.8651 - val_loss: 0.2820 - val_accuracy: 0.8994\n13/13 [==============================] - 0s 2ms/step\nAccuracy: 0.87\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.82      0.84       174\n           1       0.87      0.90      0.88       224\n\n    accuracy                           0.87       398\n   macro avg       0.87      0.86      0.86       398\nweighted avg       0.87      0.87      0.87       398\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nfrom tensorflow.keras.models import load_model\nmodel.save('ANN_advance.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:28:22.393752Z","iopub.execute_input":"2024-02-08T14:28:22.394139Z","iopub.status.idle":"2024-02-08T14:28:22.419027Z","shell.execute_reply.started":"2024-02-08T14:28:22.394109Z","shell.execute_reply":"2024-02-08T14:28:22.418048Z"},"trusted":true},"execution_count":23,"outputs":[]}]}