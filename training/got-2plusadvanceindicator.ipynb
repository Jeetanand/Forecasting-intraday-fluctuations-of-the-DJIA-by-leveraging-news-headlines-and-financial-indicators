{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7349305,"sourceType":"datasetVersion","datasetId":4267719}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import GPT2Tokenizer, GPT2Model\nimport torch\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding, Dropout\nfrom keras.optimizers import Adam\n\n# Assuming your DataFrame is named 'df'\ndf = pd.read_csv('/kaggle/input/dataset/combined_DJIA_NEWS.csv')\n# Drop unnecessary columns for training\ndf.drop(['Unnamed: 0', 'Date'], axis=1, inplace=True)\n\n# Create additional features from numerical data\ndf['Lowest_Low'] = df['Low'].rolling(window=14).min()\ndf['Highest_High'] = df['High'].rolling(window=14).max()\ndf['Stochastic_K'] = ((df['Close'] - df['Lowest_Low']) / (df['Highest_High'] - df['Lowest_Low'])) * 100\ndf['Stochastic_D'] = df['Stochastic_K'].rolling(window=3).mean()\ndf['Momentum'] = df['Close'] - df['Close'].shift(10)\ndf['Rate_of_Change'] = (df['Close'] / df['Close'].shift(10)) * 100\ndf['William_R'] = ((df['Highest_High'] - df['Close']) / (df['Highest_High'] - df['Lowest_Low'])) * -100\ndf['A/D_Oscillator'] = (df['High'] - df['Close'].shift()) / (df['High'] - df['Low'])\ndf['Close_MA_5'] = df['Close'].rolling(window=5).mean()\ndf['Disparity_5'] = ((df['Close'] - df['Close_MA_5']) / df['Close_MA_5']) * 100\ndf = df.drop(columns=['Lowest_Low', 'Highest_High', 'Close_MA_5'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-07T17:11:17.438324Z","iopub.execute_input":"2024-02-07T17:11:17.439068Z","iopub.status.idle":"2024-02-07T17:11:17.620335Z","shell.execute_reply.started":"2024-02-07T17:11:17.439034Z","shell.execute_reply":"2024-02-07T17:11:17.619161Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Load pre-trained GPT-2 tokenizer and model from Hugging Face Model Hub\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nimport numpy as np\n\nclass GPT2EmbeddingTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        embeddings = []\n        for row in X.values:\n            # Convert each element to string\n            row = [str(element) for element in row]\n\n            # Combine headlines into a single string\n            combined_headlines = \" \".join(row)\n\n            # Tokenize and get embeddings\n            inputs = tokenizer(combined_headlines, return_tensors=\"pt\", truncation=True)\n            outputs = model(**inputs)\n            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n            embeddings.append(embedding)\n\n        return np.array(embeddings)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-07T17:11:20.749621Z","iopub.execute_input":"2024-02-07T17:11:20.750640Z","iopub.status.idle":"2024-02-07T17:11:25.948106Z","shell.execute_reply.started":"2024-02-07T17:11:20.750600Z","shell.execute_reply":"2024-02-07T17:11:25.947344Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f0844110244e4a90c588f18ecd27dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ca2bb488014cf799a42eba8a04a908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84395393b0324123b7cf65892a4136c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9419bbf74e47cfa0ba3e1066485b4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462283e067344352a25254f2b7b8a5e1"}},"metadata":{}}]},{"cell_type":"code","source":"# Define the features and the target variable\ntext_features = ['Top' + str(i) for i in range(1, 26)]\nnumerical_features = ['Stochastic_K', 'Stochastic_D', 'Momentum', 'Rate_of_Change', 'William_R', 'A/D_Oscillator', 'Disparity_5']\ntarget_variable = 'Label'\n\n# Define the preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', GPT2EmbeddingTransformer(), text_features),\n        ('num', StandardScaler(), numerical_features)\n    ]\n)\n\n# Separate features and target variable in training and testing sets\nX_train = train_df[text_features + numerical_features]\ny_train = train_df[target_variable]\nX_test = test_df[text_features + numerical_features]\ny_test = test_df[target_variable]\n\n# Transform the data\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-07T17:11:33.658216Z","iopub.execute_input":"2024-02-07T17:11:33.658905Z","iopub.status.idle":"2024-02-07T17:38:17.639153Z","shell.execute_reply.started":"2024-02-07T17:11:33.658873Z","shell.execute_reply":"2024-02-07T17:38:17.638066Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of X_train_transformed:\", X_train_transformed.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:41:01.497650Z","iopub.execute_input":"2024-02-07T17:41:01.498027Z","iopub.status.idle":"2024-02-07T17:41:01.503876Z","shell.execute_reply.started":"2024-02-07T17:41:01.497998Z","shell.execute_reply":"2024-02-07T17:41:01.502657Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Shape of X_train_transformed: (1590, 775)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**LSTM**","metadata":{}},{"cell_type":"code","source":"# Reshape the data for LSTM\nX_train_transformed = X_train_transformed.reshape((X_train_transformed.shape[0], X_train_transformed.shape[1], 1))\nX_test_transformed = X_test_transformed.reshape((X_test_transformed.shape[0], X_test_transformed.shape[1], 1))\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(X_train_transformed.shape[1], 1), return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Train the model\nmodel.fit(X_train_transformed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Evaluate the model\npredictions = (model.predict(X_test_transformed) > 0.5).astype(int)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-07T17:43:42.422842Z","iopub.execute_input":"2024-02-07T17:43:42.423370Z","iopub.status.idle":"2024-02-07T17:44:28.247818Z","shell.execute_reply.started":"2024-02-07T17:43:42.423339Z","shell.execute_reply":"2024-02-07T17:44:28.246799Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 7s 69ms/step - loss: nan - accuracy: 0.4724 - val_loss: nan - val_accuracy: 0.5346\nEpoch 2/10\n45/45 [==============================] - 2s 46ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 3/10\n45/45 [==============================] - 2s 46ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 4/10\n45/45 [==============================] - 2s 47ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 5/10\n45/45 [==============================] - 2s 47ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 6/10\n45/45 [==============================] - 2s 47ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 7/10\n45/45 [==============================] - 2s 47ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 8/10\n45/45 [==============================] - 2s 46ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 9/10\n45/45 [==============================] - 2s 47ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 10/10\n45/45 [==============================] - 2s 46ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\n13/13 [==============================] - 1s 29ms/step\nAccuracy: 0.44\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Assuming your X_train_transformed and X_test_transformed are 3D arrays\n# (number of samples, number of timesteps, number of features)\n# You can adjust input_shape based on your data\n\n# Define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(X_train_transformed.shape[1], X_train_transformed.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_transformed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Evaluate the model\npredictions = (model.predict(X_test_transformed) > 0.5).astype(int)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:45:10.021111Z","iopub.execute_input":"2024-02-07T17:45:10.021802Z","iopub.status.idle":"2024-02-07T17:45:26.244873Z","shell.execute_reply.started":"2024-02-07T17:45:10.021773Z","shell.execute_reply":"2024-02-07T17:45:26.243883Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 5s 40ms/step - loss: nan - accuracy: 0.4626 - val_loss: nan - val_accuracy: 0.5346\nEpoch 2/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 3/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 4/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 5/10\n45/45 [==============================] - 1s 25ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 6/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 7/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 8/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 9/10\n45/45 [==============================] - 1s 24ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\nEpoch 10/10\n45/45 [==============================] - 1s 25ms/step - loss: nan - accuracy: 0.4647 - val_loss: nan - val_accuracy: 0.5346\n13/13 [==============================] - 1s 10ms/step\nAccuracy: 0.44\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**LogisticRegression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='mean')\nX_train_flattened = imputer.fit_transform(X_train_flattened)\nX_test_flattened = imputer.transform(X_test_flattened)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_flattened and X_test_flattened are your feature matrices\n# and y_train is your target variable\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Define the Logistic Regression model\nlogreg_model = LogisticRegression(random_state=42)\n\n# Train the model\nlogreg_model.fit(X_train_imputed, y_train)\n\n# Make predictions on training set\nlogreg_train_predictions = logreg_model.predict(X_train_imputed)\n\n# Make predictions on test set\nlogreg_test_predictions = logreg_model.predict(X_test_imputed)\n\n# Evaluate the model on training set\ntrain_accuracy = accuracy_score(y_train, logreg_train_predictions)\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\n\n# Evaluate the model on test set\ntest_accuracy = accuracy_score(y_test, logreg_test_predictions)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display classification report for test set\nprint(\"Classification Report (Test Set):\")\nprint(classification_report(y_test, logreg_test_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:51:12.235055Z","iopub.execute_input":"2024-02-07T17:51:12.235951Z","iopub.status.idle":"2024-02-07T17:51:12.456442Z","shell.execute_reply.started":"2024-02-07T17:51:12.235917Z","shell.execute_reply":"2024-02-07T17:51:12.455149Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Training Accuracy: 0.92\nTest Accuracy: 0.87\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n           0       0.85      0.85      0.85       174\n           1       0.88      0.88      0.88       224\n\n    accuracy                           0.87       398\n   macro avg       0.86      0.87      0.86       398\nweighted avg       0.87      0.87      0.87       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Define the SVM model\nsvm_model = SVC(random_state=42)\n\n# Train the model\nsvm_model.fit(X_train_imputed, y_train)\n\n# Make predictions on training set\nsvm_train_predictions = svm_model.predict(X_train_imputed)\n\n# Make predictions on test set\nsvm_test_predictions = svm_model.predict(X_test_imputed)\n\n# Evaluate the model on training set\ntrain_accuracy = accuracy_score(y_train, svm_train_predictions)\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\n\n# Evaluate the model on test set\ntest_accuracy = accuracy_score(y_test, svm_test_predictions)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display classification report for test set\nprint(\"Classification Report (Test Set):\")\nprint(classification_report(y_test, svm_test_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:54:45.526919Z","iopub.execute_input":"2024-02-07T17:54:45.527629Z","iopub.status.idle":"2024-02-07T17:54:48.567353Z","shell.execute_reply.started":"2024-02-07T17:54:45.527593Z","shell.execute_reply":"2024-02-07T17:54:48.566345Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Training Accuracy: 0.53\nTest Accuracy: 0.56\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       174\n           1       0.56      1.00      0.72       224\n\n    accuracy                           0.56       398\n   macro avg       0.28      0.50      0.36       398\nweighted avg       0.32      0.56      0.41       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**svm with window size 25**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_flattened and X_test_flattened are your feature matrices\n# and y_train is your target variable\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Set the training window size\nwindow_size = 25\n\n# Initialize the SVM model\nsvm_model = SVC(random_state=42)\n\n# Train the model with a sliding window\nfor i in range(window_size, len(X_train_imputed)):\n    X_train_window = X_train_imputed[i - window_size:i]\n    y_train_window = y_train[i - window_size:i]\n    svm_model.fit(X_train_window, y_train_window)\n\n# Make predictions on training set\nsvm_train_predictions = svm_model.predict(X_train_imputed)\n\n# Make predictions on test set\nsvm_test_predictions = svm_model.predict(X_test_imputed)\n\n# Evaluate the model on training set\ntrain_accuracy = accuracy_score(y_train, svm_train_predictions)\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\n\n# Evaluate the model on test set\ntest_accuracy = accuracy_score(y_test, svm_test_predictions)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display classification report for test set\nprint(\"Classification Report (Test Set):\")\nprint(classification_report(y_test, svm_test_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:56:28.640540Z","iopub.execute_input":"2024-02-07T17:56:28.640906Z","iopub.status.idle":"2024-02-07T17:56:31.483804Z","shell.execute_reply.started":"2024-02-07T17:56:28.640878Z","shell.execute_reply":"2024-02-07T17:56:31.482868Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Training Accuracy: 0.47\nTest Accuracy: 0.44\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61       174\n           1       0.00      0.00      0.00       224\n\n    accuracy                           0.44       398\n   macro avg       0.22      0.50      0.30       398\nweighted avg       0.19      0.44      0.27       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**CNN-LSTM**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Reshape back to the original shape\nX_train_imputed = X_train_imputed.reshape(X_train_transformed.shape)\nX_test_imputed = X_test_imputed.reshape(X_test_transformed.shape)\n\n# CNN-LSTM model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_imputed.shape[1], X_train_imputed.shape[2])))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_imputed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Make predictions\npredictions = (model.predict(X_test_imputed) > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T18:00:35.098793Z","iopub.execute_input":"2024-02-07T18:00:35.099903Z","iopub.status.idle":"2024-02-07T18:04:44.700101Z","shell.execute_reply.started":"2024-02-07T18:00:35.099829Z","shell.execute_reply":"2024-02-07T18:04:44.698929Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 28s 550ms/step - loss: 4.2450 - accuracy: 0.6478 - val_loss: 0.6658 - val_accuracy: 0.6541\nEpoch 2/10\n45/45 [==============================] - 24s 534ms/step - loss: 0.6636 - accuracy: 0.6653 - val_loss: 0.6655 - val_accuracy: 0.6604\nEpoch 3/10\n45/45 [==============================] - 25s 549ms/step - loss: 0.6628 - accuracy: 0.6618 - val_loss: 0.6651 - val_accuracy: 0.6415\nEpoch 4/10\n45/45 [==============================] - 24s 536ms/step - loss: 0.6619 - accuracy: 0.6597 - val_loss: 0.6648 - val_accuracy: 0.6415\nEpoch 5/10\n45/45 [==============================] - 24s 539ms/step - loss: 0.6611 - accuracy: 0.6583 - val_loss: 0.6642 - val_accuracy: 0.6352\nEpoch 6/10\n45/45 [==============================] - 24s 543ms/step - loss: 0.6602 - accuracy: 0.6583 - val_loss: 0.6638 - val_accuracy: 0.6352\nEpoch 7/10\n45/45 [==============================] - 25s 548ms/step - loss: 0.6593 - accuracy: 0.6618 - val_loss: 0.6633 - val_accuracy: 0.6352\nEpoch 8/10\n45/45 [==============================] - 25s 545ms/step - loss: 0.6584 - accuracy: 0.6618 - val_loss: 0.6625 - val_accuracy: 0.6289\nEpoch 9/10\n45/45 [==============================] - 25s 551ms/step - loss: 0.6574 - accuracy: 0.6590 - val_loss: 0.6616 - val_accuracy: 0.6352\nEpoch 10/10\n45/45 [==============================] - 24s 541ms/step - loss: 0.6565 - accuracy: 0.6590 - val_loss: 0.6608 - val_accuracy: 0.6289\n13/13 [==============================] - 1s 74ms/step\nAccuracy: 0.71\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.75      0.50      0.60       174\n           1       0.69      0.87      0.77       224\n\n    accuracy                           0.71       398\n   macro avg       0.72      0.69      0.69       398\nweighted avg       0.72      0.71      0.70       398\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**CNN-GRU**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_transformed, X_test_transformed, y_train, and y_test are your data\n# X_train_transformed and X_test_transformed should have shape (samples, timesteps, features)\n\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Reshape back to the original shape\nX_train_imputed = X_train_imputed.reshape(X_train_transformed.shape)\nX_test_imputed = X_test_imputed.reshape(X_test_transformed.shape)\n\n# CNN-GRU model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_imputed.shape[1], X_train_imputed.shape[2])))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(GRU(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_imputed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Make predictions\npredictions = (model.predict(X_test_imputed) > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T18:05:56.292271Z","iopub.execute_input":"2024-02-07T18:05:56.292983Z","iopub.status.idle":"2024-02-07T18:10:32.734079Z","shell.execute_reply.started":"2024-02-07T18:05:56.292947Z","shell.execute_reply":"2024-02-07T18:10:32.733095Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 31s 628ms/step - loss: 0.6155 - accuracy: 0.6758 - val_loss: 0.5182 - val_accuracy: 0.7547\nEpoch 2/10\n45/45 [==============================] - 27s 608ms/step - loss: 0.4704 - accuracy: 0.8148 - val_loss: 0.3466 - val_accuracy: 0.8742\nEpoch 3/10\n45/45 [==============================] - 27s 606ms/step - loss: 0.3341 - accuracy: 0.8526 - val_loss: 0.3102 - val_accuracy: 0.8805\nEpoch 4/10\n45/45 [==============================] - 27s 607ms/step - loss: 0.3073 - accuracy: 0.8637 - val_loss: 0.2847 - val_accuracy: 0.8994\nEpoch 5/10\n45/45 [==============================] - 27s 601ms/step - loss: 0.2793 - accuracy: 0.8728 - val_loss: 0.2442 - val_accuracy: 0.9057\nEpoch 6/10\n45/45 [==============================] - 27s 607ms/step - loss: 0.2636 - accuracy: 0.8819 - val_loss: 0.2413 - val_accuracy: 0.9057\nEpoch 7/10\n45/45 [==============================] - 27s 598ms/step - loss: 0.2646 - accuracy: 0.8770 - val_loss: 0.2277 - val_accuracy: 0.9057\nEpoch 8/10\n45/45 [==============================] - 27s 597ms/step - loss: 0.2568 - accuracy: 0.8812 - val_loss: 0.2355 - val_accuracy: 0.8868\nEpoch 9/10\n45/45 [==============================] - 27s 597ms/step - loss: 0.2545 - accuracy: 0.8847 - val_loss: 0.2183 - val_accuracy: 0.9245\nEpoch 10/10\n45/45 [==============================] - 27s 603ms/step - loss: 0.2540 - accuracy: 0.8875 - val_loss: 0.2164 - val_accuracy: 0.9371\n13/13 [==============================] - 1s 77ms/step\nAccuracy: 0.89\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.83      0.87       174\n           1       0.88      0.93      0.90       224\n\n    accuracy                           0.89       398\n   macro avg       0.89      0.88      0.88       398\nweighted avg       0.89      0.89      0.89       398\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**ANN**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\n\n# Assuming X_train_transformed, X_test_transformed, y_train, and y_test are your data\n# X_train_transformed and X_test_transformed should have shape (samples, features)\n\n# Flatten the data\nX_train_flattened = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flattened = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Impute NaN values\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train_flattened)\nX_test_imputed = imputer.transform(X_test_flattened)\n\n# Create a simple ANN model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X_train_imputed.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_imputed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Make predictions\npredictions = (model.predict(X_test_imputed) > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T18:11:09.921248Z","iopub.execute_input":"2024-02-07T18:11:09.921987Z","iopub.status.idle":"2024-02-07T18:11:13.683750Z","shell.execute_reply.started":"2024-02-07T18:11:09.921955Z","shell.execute_reply":"2024-02-07T18:11:13.682877Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 2s 9ms/step - loss: 0.7163 - accuracy: 0.6059 - val_loss: 0.6324 - val_accuracy: 0.5660\nEpoch 2/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.5582 - accuracy: 0.7149 - val_loss: 0.6122 - val_accuracy: 0.6226\nEpoch 3/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.5257 - accuracy: 0.7407 - val_loss: 0.4689 - val_accuracy: 0.8113\nEpoch 4/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8288 - val_loss: 0.4667 - val_accuracy: 0.7421\nEpoch 5/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.3957 - accuracy: 0.8379 - val_loss: 0.3738 - val_accuracy: 0.8805\nEpoch 6/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.3562 - accuracy: 0.8616 - val_loss: 0.3440 - val_accuracy: 0.8868\nEpoch 7/10\n45/45 [==============================] - 0s 4ms/step - loss: 0.3333 - accuracy: 0.8616 - val_loss: 0.3334 - val_accuracy: 0.8553\nEpoch 8/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.3335 - accuracy: 0.8658 - val_loss: 0.2963 - val_accuracy: 0.8868\nEpoch 9/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.3013 - accuracy: 0.8672 - val_loss: 0.2855 - val_accuracy: 0.8742\nEpoch 10/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.2702 - accuracy: 0.8896 - val_loss: 0.2730 - val_accuracy: 0.9057\n13/13 [==============================] - 0s 2ms/step\nAccuracy: 0.87\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.80      0.85       174\n           1       0.86      0.93      0.89       224\n\n    accuracy                           0.87       398\n   macro avg       0.88      0.87      0.87       398\nweighted avg       0.88      0.87      0.87       398\n\n","output_type":"stream"}]}]}