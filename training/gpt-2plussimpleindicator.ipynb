{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import GPT2Tokenizer, GPT2Model\nimport torch\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding, Dropout\nfrom keras.optimizers import Adam\n\n# Assuming your DataFrame is named 'df'\ndf = pd.read_csv('/kaggle/input/dataset/combined_DJIA_NEWS.csv')\n# Drop unnecessary columns for training\ndf.drop(['Unnamed: 0', 'Date'], axis=1, inplace=True)\n\n# Split the data into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Load pre-trained GPT-2 tokenizer and model from Hugging Face Model Hub\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\nimport numpy as np\n\nclass GPT2EmbeddingTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        embeddings = []\n        for row in X.values:\n            # Convert each element to string\n            row = [str(element) for element in row]\n\n            # Combine headlines into a single string\n            combined_headlines = \" \".join(row)\n\n            # Tokenize and get embeddings\n            inputs = tokenizer(combined_headlines, return_tensors=\"pt\", truncation=True)\n            outputs = model(**inputs)\n            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n            embeddings.append(embedding)\n\n        return np.array(embeddings)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:52:49.909833Z","iopub.execute_input":"2024-02-07T15:52:49.910221Z","iopub.status.idle":"2024-02-07T15:52:50.576258Z","shell.execute_reply.started":"2024-02-07T15:52:49.910189Z","shell.execute_reply":"2024-02-07T15:52:50.575249Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Separate features and labels\nX_train = train_df.drop('Label', axis=1)\ny_train = train_df['Label']\nX_test = test_df.drop('Label', axis=1)\ny_test = test_df['Label']\n\n# Combine transformers using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', GPT2EmbeddingTransformer(), [f'Top{i}' for i in range(1, 26)]),\n        ('num', StandardScaler(), ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n    ]\n)\n\n# Transform the data\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:52:54.267555Z","iopub.execute_input":"2024-02-07T15:52:54.267915Z","iopub.status.idle":"2024-02-07T16:18:15.801283Z","shell.execute_reply.started":"2024-02-07T15:52:54.267885Z","shell.execute_reply":"2024-02-07T16:18:15.800496Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**LSTM**","metadata":{}},{"cell_type":"code","source":"# Reshape X_train_transformed to have a third dimension\nX_train_transformed = X_train_transformed.reshape(X_train_transformed.shape[0], 1, X_train_transformed.shape[1])\n\n# Print the shape of transformed data for debugging\nprint(\"Shape of X_train_transformed:\", X_train_transformed.shape)\n\n# Define the LSTM model\nmodel = Sequential()\n# Adjust input shape based on the actual shape of X_train_transformed\ninput_shape = (X_train_transformed.shape[1], X_train_transformed.shape[2])\nprint(\"LSTM Input Shape:\", input_shape)\nmodel.add(LSTM(100, input_shape=input_shape))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_transformed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T16:45:58.999968Z","iopub.execute_input":"2024-02-07T16:45:59.000926Z","iopub.status.idle":"2024-02-07T16:46:06.133102Z","shell.execute_reply.started":"2024-02-07T16:45:59.000895Z","shell.execute_reply":"2024-02-07T16:46:06.132257Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Shape of X_train_transformed: (1590, 1, 774)\nLSTM Input Shape: (1, 774)\nEpoch 1/10\n13/45 [=======>......................] - ETA: 0s - loss: 0.6975 - accuracy: 0.5409  ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707324363.368577      87 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"45/45 [==============================] - 4s 16ms/step - loss: 0.6945 - accuracy: 0.5353 - val_loss: 0.6993 - val_accuracy: 0.4654\nEpoch 2/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5353 - val_loss: 0.7008 - val_accuracy: 0.4654\nEpoch 3/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5353 - val_loss: 0.7002 - val_accuracy: 0.4654\nEpoch 4/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5360 - val_loss: 0.7020 - val_accuracy: 0.4654\nEpoch 5/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6992 - val_accuracy: 0.4528\nEpoch 6/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6911 - accuracy: 0.5318 - val_loss: 0.6981 - val_accuracy: 0.4717\nEpoch 7/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6904 - accuracy: 0.5374 - val_loss: 0.7032 - val_accuracy: 0.4654\nEpoch 8/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.5444 - val_loss: 0.7012 - val_accuracy: 0.4654\nEpoch 9/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6894 - accuracy: 0.5381 - val_loss: 0.7046 - val_accuracy: 0.4654\nEpoch 10/10\n45/45 [==============================] - 0s 5ms/step - loss: 0.6892 - accuracy: 0.5388 - val_loss: 0.6978 - val_accuracy: 0.4654\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7f8a66072290>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model on the test set\nX_test_transformed_reshaped = X_test_transformed.reshape(X_test_transformed.shape[0], 1, X_test_transformed.shape[1])\naccuracy = model.evaluate(X_test_transformed_reshaped, y_test)[1]\n\n# Print accuracy\nprint(f\"Accuracy on the test set: {accuracy:.2%}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T16:46:57.663155Z","iopub.execute_input":"2024-02-07T16:46:57.663827Z","iopub.status.idle":"2024-02-07T16:46:57.795036Z","shell.execute_reply.started":"2024-02-07T16:46:57.663792Z","shell.execute_reply":"2024-02-07T16:46:57.794131Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"13/13 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.5653\nAccuracy on the test set: 56.53%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Simple Neural Network**","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\n\n# Define the model\nmodel = Sequential()\n\n# Flatten the input (assuming it is a 2D input)\nmodel.add(Flatten(input_shape=(X_train_transformed.shape[1], X_train_transformed.shape[2])))\n\n# Add Dense layers\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_transformed, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Evaluate the model on the test set\naccuracy = model.evaluate(X_test_transformed, y_test)[1]\n\n# Print accuracy\nprint(f\"Accuracy on the test set: {accuracy:.2%}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T16:49:26.701297Z","iopub.execute_input":"2024-02-07T16:49:26.702039Z","iopub.status.idle":"2024-02-07T16:49:31.404281Z","shell.execute_reply.started":"2024-02-07T16:49:26.702007Z","shell.execute_reply":"2024-02-07T16:49:31.403406Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10\n45/45 [==============================] - 3s 8ms/step - loss: 5.0176 - accuracy: 0.5045 - val_loss: 0.8066 - val_accuracy: 0.4591\nEpoch 2/10\n45/45 [==============================] - 0s 4ms/step - loss: 4.6044 - accuracy: 0.4892 - val_loss: 0.8031 - val_accuracy: 0.4780\nEpoch 3/10\n45/45 [==============================] - 0s 4ms/step - loss: 3.7044 - accuracy: 0.5164 - val_loss: 0.7163 - val_accuracy: 0.4843\nEpoch 4/10\n45/45 [==============================] - 0s 4ms/step - loss: 3.1366 - accuracy: 0.5248 - val_loss: 0.7153 - val_accuracy: 0.4780\nEpoch 5/10\n45/45 [==============================] - 0s 4ms/step - loss: 2.9307 - accuracy: 0.4927 - val_loss: 0.7248 - val_accuracy: 0.4403\nEpoch 6/10\n45/45 [==============================] - 0s 5ms/step - loss: 2.4580 - accuracy: 0.5087 - val_loss: 0.7438 - val_accuracy: 0.4654\nEpoch 7/10\n45/45 [==============================] - 0s 4ms/step - loss: 2.2855 - accuracy: 0.4913 - val_loss: 0.7304 - val_accuracy: 0.4654\nEpoch 8/10\n45/45 [==============================] - 0s 4ms/step - loss: 1.9584 - accuracy: 0.4934 - val_loss: 0.7279 - val_accuracy: 0.4654\nEpoch 9/10\n45/45 [==============================] - 0s 4ms/step - loss: 1.6549 - accuracy: 0.5024 - val_loss: 0.7225 - val_accuracy: 0.4654\nEpoch 10/10\n45/45 [==============================] - 0s 4ms/step - loss: 1.5295 - accuracy: 0.5143 - val_loss: 0.7200 - val_accuracy: 0.4654\n13/13 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5628\nAccuracy on the test set: 56.28%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"# Flatten the time dimension\nX_train_flat = X_train_transformed.reshape(X_train_transformed.shape[0], -1)\nX_test_flat = X_test_transformed.reshape(X_test_transformed.shape[0], -1)\n\n# Create a Logistic Regression model\nlogreg_model = LogisticRegression(random_state=42)\n\n# Train the model\nlogreg_model.fit(X_train_flat, y_train)\n\n# Make predictions\nlogreg_predictions = logreg_model.predict(X_test_flat)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, logreg_predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, logreg_predictions))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T16:51:57.536331Z","iopub.execute_input":"2024-02-07T16:51:57.537241Z","iopub.status.idle":"2024-02-07T16:51:57.735637Z","shell.execute_reply.started":"2024-02-07T16:51:57.537206Z","shell.execute_reply":"2024-02-07T16:51:57.729557Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Accuracy: 0.53\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.46      0.42      0.44       174\n           1       0.58      0.62      0.60       224\n\n    accuracy                           0.53       398\n   macro avg       0.52      0.52      0.52       398\nweighted avg       0.53      0.53      0.53       398\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Initialize the SVM classifier\nsvm_model = SVC(kernel='linear', random_state=42)\n\n# Train the SVM model\nsvm_model.fit(X_train_transformed.reshape(X_train_transformed.shape[0], -1), y_train)\n\n# Make predictions on the test set\nsvm_predictions = svm_model.predict(X_test_transformed.reshape(X_test_transformed.shape[0], -1))\n\n# Evaluate the SVM model\naccuracy_svm = accuracy_score(y_test, svm_predictions)\nprint(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n\n# Display classification report\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_test, svm_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T16:53:09.897598Z","iopub.execute_input":"2024-02-07T16:53:09.897957Z","iopub.status.idle":"2024-02-07T16:53:12.269818Z","shell.execute_reply.started":"2024-02-07T16:53:09.897928Z","shell.execute_reply":"2024-02-07T16:53:12.268862Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"SVM Accuracy: 0.67\nSVM Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.64      0.59      0.61       174\n           1       0.70      0.74      0.72       224\n\n    accuracy                           0.67       398\n   macro avg       0.67      0.66      0.67       398\nweighted avg       0.67      0.67      0.67       398\n\n","output_type":"stream"}]}]}